{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Approximate String Matching using TF-IDF over Large Dataset - Explanation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7fB3SL599ynB",
        "4hCIr3CM92Yw",
        "EnUiYJhoiyhV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ignacio-Ibarra/Approximate-String-Matching/blob/main/Approximate_String_Matching_using_TF_IDF_over_Large_Dataset_Explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8atkFWki7UW_"
      },
      "source": [
        "# Similitud entre Strings usando TF-IDF en Dataset de +11M de filas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fB3SL599ynB"
      },
      "source": [
        "## Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGiAp0z27kow"
      },
      "source": [
        "El siguiente notebook es un trabajo realizado por el [CEP](https://www.argentina.gob.ar/produccion/cep) con el objetivo de encontrar una mejora las fuentes de información existentes aceca de las empresas que realizaron exportaciones a la Argentina, entre XXXX y XXXX. \n",
        "\n",
        "Se cuenta con una base de datos de más de 11M de filas con los nombres de compañias que han realizado ventas a la Argentina, pero cuyo nombre se encuentra por momentos mal escrito, escrito de varias formas distintas, con puntuaciones, distinos formatos, etc. El objetivo es reducir la cantidad de nombres distintos a la minima cantidad posible, limpiando los nombres sucios. \n",
        "\n",
        "Para ello, luego de la lectura bibliográfica correspondiente y el abordaje de distintas alternativas, se utilizó la técnica **TF-IDF** como método de vectorización y la distancia **_cosine similarity_** como forma de evaluación de la similitud entre los nombres de las empresas mediante. Previo a ello se realizó un preprocesamiento general de la base mediante el uso de **_regular expressions_**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hCIr3CM92Yw"
      },
      "source": [
        "## ¿Cómo se aborda este tipo de problemas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je4MCk2aK0yp"
      },
      "source": [
        "Según bibliografía consultada uno de los método para comparar obtener similitud entre palabras o frases (en nuestro caso nombres de empresas) en datasets grandes es el metodo TF-IDF. \n",
        "\n",
        "**¿Por qué no la distancia de Levenshtein?**\n",
        "\n",
        "La distancia de Levenshtein (también conocida como la distancia de edición) es uno de los métodos más usados para analizar la similitud entre dos strings. La similitud que analiza es qué tan parecidas son dos secuencias de caracteres y esta está medida en base a la cantidad de cambios que habría que hacerle al string A para que se convierta en string B. \n",
        "\n",
        "El problema que tenemos planteado es el de comparar cada uno de los nombres de las empresas que tenemos contra los otros N-1 otros nombres. Si bien es muy efectiva la medición de la similitud mediante esta distancia, tiene el problema de que no escala bien para datasets grandes, dado que el problema es de complejidad $\\Omega(n^2)$. \n",
        "\n",
        "Es necesario encontrar otro método que permita resolver el problema de manera más eficiente para casos de _big data_. \n",
        "\n",
        "**¿Qué es TF-IDF?** \n",
        "\n",
        "TF-IDF (Term Frecuency-Invers Document Frecuency) es una técnica de consultas de información en documentos perteneciente al campo computacional conocido como [Information Retrieval](https://en.wikipedia.org/wiki/Information_retrieval). Este tipo de consulta es rankeada porque la búsqueda de documentos a partir de términos las realiza en base a la importancia que tiene un término en el documento y en base a la inversa de los documenos que contienen dicho documento. \n",
        "\n",
        "* Term Frecuency (TF): \n",
        "\n",
        "\\begin{equation}\n",
        "TF_{i,j} =\\frac{Frecuencia Absoluta Termino_{i} Documento_{j}}{Cantidad Terminos Documento_{j}} = f_{i,j}\n",
        "\\end{equation}\n",
        "\n",
        "* Inverse Document Frecuency (IDF): \n",
        "\n",
        "\\begin{equation}\n",
        "IDF_{i} =\\log(\\frac{N+1}{Cantidad Documentos Término_{i}})=\\log(\\frac{N+1}{d_{i}})\n",
        "\\end{equation}\n",
        "\n",
        "* TF-IDF: \n",
        "\n",
        "\\begin{equation}\n",
        "TF-IDF_{i,j} =f_{i,j} .\\log(\\frac{N+1}{d_{i}})\n",
        "\\end{equation}\n",
        "\n",
        "* Algunos plantean como opción para restarle importancia a TF: \n",
        "\n",
        "\\begin{equation}\n",
        "TF-IDF_{i,j} =\\log(1+f_{i,j}).\\log(\\frac{N+1}{d_{i}})\n",
        "\\end{equation}\n",
        "\n",
        "* Otra opcion es: \n",
        "\n",
        "\\begin{equation}\n",
        "TF-IDF_{i,j} =\\log(1+\\log(1+f_{i,j})).\\log(\\frac{N+1}{d_{i}})\n",
        "\\end{equation}\n",
        "\n",
        "Considerando un documento que contiene 100 palabras en el cual la palabra gato aparece 3 veces. TF = (3/100) = 0.03. Ahora, asumiendo que en total hay 9.999.999 documentos y la palabra gato aparece en mil de esos documentos. IDF = log(9.999.999 + 1 / 1,000) = 4. Por lo tanto TF-IDF = 0.03*4 = 0.12. El número total de documentos fue elegido para que el logaritmo dé un nro redondo. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7iWPR6pXgh6"
      },
      "source": [
        "Con la libería [scikit-learn](https://scikit-learn.org/stable/) se puede rapidamente calcular para cada termino de cada documento de toda una colección el respectivo valor TF-IDF. El cálculo que realiza el método `TfidfVectorizer` no es el estándar, ya que divide el valor por la norma del vector que está analizando. La matriz que se genera siempre en estos casos es de dimensión n x m siendo n la cantidad de documentos y m la cantidad de términos de toda la colección. Cada fila de la matriz estará compuesta por los respectivos valores tf-idf en aquellas columnas de los términos que se encuentren dentro del documento y zeros en todos las columnas de los términos que no están incluidos dentro del documento. Lo que arrojará una matriz esparsa (con muuuuchos ceros). \n",
        "Se puede decir que dos vectores poseen cierta similitud si ambos \"apuntan en la misma dirección\". \n",
        "De esta manera logramos traducir un vector de términos a un vector de números. Ejemplo, supongamos que tenemos tres documentos, cada uno con tres palabras.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "rs0IMLO9gyl2",
        "outputId": "61b3a832-1687-40ea-b24c-ffa7603ec501"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        " \n",
        "documentos = (\n",
        "\"La casa del perro está rota hace tiempo\",\n",
        "\"El perro tiene la pata rota ahora\",\n",
        "\"El perro no tiene la casa rota\",\n",
        "\"El no perro perro perro\")\n",
        " \n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documentos)\n",
        "print(\"Dimensiones de la Matriz: \", tfidf_matrix.shape)\n",
        "print('**'*25)\n",
        "print('Traducido a Matriz TF-IDF sería: \\n')\n",
        "words = tfidf_vectorizer.get_feature_names()\n",
        "pd.DataFrame(tfidf_matrix.toarray(), columns=words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensiones de la Matriz:  (4, 13)\n",
            "**************************************************\n",
            "Traducido a Matriz TF-IDF sería: \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ahora</th>\n",
              "      <th>casa</th>\n",
              "      <th>del</th>\n",
              "      <th>el</th>\n",
              "      <th>está</th>\n",
              "      <th>hace</th>\n",
              "      <th>la</th>\n",
              "      <th>no</th>\n",
              "      <th>pata</th>\n",
              "      <th>perro</th>\n",
              "      <th>rota</th>\n",
              "      <th>tiempo</th>\n",
              "      <th>tiene</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.329977</td>\n",
              "      <td>0.418533</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.418533</td>\n",
              "      <td>0.418533</td>\n",
              "      <td>0.267144</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.218408</td>\n",
              "      <td>0.267144</td>\n",
              "      <td>0.418533</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.492895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.314609</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.314609</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.492895</td>\n",
              "      <td>0.257213</td>\n",
              "      <td>0.314609</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.388604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.430157</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.348249</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.348249</td>\n",
              "      <td>0.430157</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.284716</td>\n",
              "      <td>0.348249</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.430157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.342164</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.422641</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.839225</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      ahora      casa       del  ...      rota    tiempo     tiene\n",
              "0  0.000000  0.329977  0.418533  ...  0.267144  0.418533  0.000000\n",
              "1  0.492895  0.000000  0.000000  ...  0.314609  0.000000  0.388604\n",
              "2  0.000000  0.430157  0.000000  ...  0.348249  0.000000  0.430157\n",
              "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[4 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd5_Bqh8khPH"
      },
      "source": [
        "**¿Como evaluamos ahora la similitud?**\n",
        "\n",
        "Para definir la similitud entre dos documentos, para nuestro problema sería definir la similitud entre dos strings que indican nombres de empresas, es necesario introducir el concepto de producto escalar. Sean dos vectores $\\vec{a} = (a_1, a_2, a_3, \\ldots)$ y $\\vec{b} = (b_1, b_2, b_3, \\ldots)$, el producto escalar se define como:\n",
        "\n",
        "\\begin{equation}\n",
        "\\vec{a}\\cdot\\vec{b} = \\sum_{n=1}^{n}a_{i}.b_{i}\n",
        "\\end{equation}\n",
        "\n",
        "Desde un punto de visa geométrico el producto escalar es:\n",
        "\n",
        "\\begin{equation}\n",
        "\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\|\\|\\vec{b}\\|\\cos{\\theta} \n",
        "\\end{equation}\n",
        "\n",
        "Acomodando la ecuación tenemos que: \n",
        "\n",
        "\\begin{equation}\n",
        "\\vec{a} \\cdot \\vec{b} = \\|\\vec{b}\\|\\|\\vec{a}\\|\\cos{\\theta} \n",
        "\\end{equation}\n",
        "\n",
        "Siendo $\\|\\vec{a}\\|\\cos{\\theta} $ la proyección del vector $\\vec{a}$ en el vector $\\vec{b}$. \n",
        "\n",
        "![](https://blog.christianperone.com/wp-content/uploads/2013/09/Dot_Product.png)\n",
        "\n",
        "Cuanto más similares el ángulo $\\theta$ es cada vez menor. Para valores de $\\theta$ cercanos a $0^o$ el $\\cos{\\theta}$ tiende a $1$. Para valores cercanos a $90^o$ el valor de $\\cos{\\theta}$ tiende a $0$. Para valores cercanos a $180^o$ el valor tiende a $-1$ y para valores cercanos a $270^o$ tiende a $0$ nuevamente. Cuando el ángulo de dos vectores es igual a $90^o$ o $270^o$ se dice que los dos vectores son ortogonales. \n",
        "\n",
        "Por lo tanto para evaluar la similitud entre dos vectores tenemos que despejar la ecuacion: \n",
        "\n",
        "\\begin{equation}\n",
        "\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\|\\|\\vec{b}\\|\\cos{\\theta} \\\\ \\\\  \\cos{\\theta} = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\|\\|\\vec{b}\\|}\n",
        "\\end{equation}\n",
        "\n",
        "Vectores muy opuestos deberían tener un _cosine similarity_ tendiendo a -1, vectores muy similaeres deberían tener un score tendiendo a 1.\n",
        " \n",
        "![](https://blog.christianperone.com/wp-content/uploads/2013/09/cosinesimilarityfq1.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQZpWqSpS24w"
      },
      "source": [
        "Para el ejemplo dado el coseno del ángulo entre el primer vector y el resto de los vectores es:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW7gaP9kTCgy",
        "outputId": "401ffadc-868c-49d2-d4d9-c7151bf65739"
      },
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        " \n",
        "def cos_between_vectors(a, b): \n",
        "  return dot(a, b.transpose())\n",
        " \n",
        " \n",
        "a = tfidf_matrix.todense()[0,:]\n",
        "b = tfidf_matrix.todense()[1,:]\n",
        "c = tfidf_matrix.todense()[2,:]\n",
        "d = tfidf_matrix.todense()[3,:]\n",
        " \n",
        "N = len(tfidf_matrix.todense())\n",
        "print('Cantidad de veces que hay que calcular el Cosine Similarity :', int((N*(N-1))/2))\n",
        " \n",
        " \n",
        "a_b = cos_between_vectors(a,b)\n",
        "print(\"\\nCosine_Similarity entre vector a y b: \", a_b)\n",
        "a_c = cos_between_vectors(a,c)\n",
        "print(\"\\nCosine_Similarity entre vector a y c: \", a_c)\n",
        "a_d = cos_between_vectors(a,d)\n",
        "print(\"\\nCosine_Similarity entre vector a y d: \", a_d)\n",
        "b_c = cos_between_vectors(b,c)\n",
        "print(\"\\nCosine_Similarity entre vector b y c: \", b_c)\n",
        "b_d = cos_between_vectors(b,d)\n",
        "print(\"\\nCosine_Similarity entre vector b y d: \", b_d)\n",
        "c_d = cos_between_vectors(c,d)\n",
        "print(\"\\nCosine_Similarity entre vector c y d: \", c_d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cantidad de veces que hay que calcular el Cosine Similarity : 6\n",
            "\n",
            "Cosine_Similarity entre vector a y b:  [[0.22426947]]\n",
            "\n",
            "Cosine_Similarity entre vector a y c:  [[0.39019161]]\n",
            "\n",
            "Cosine_Similarity entre vector a y d:  [[0.18329353]]\n",
            "\n",
            "Cosine_Similarity entre vector b y c:  [[0.56908022]]\n",
            "\n",
            "Cosine_Similarity entre vector b y d:  [[0.32350765]]\n",
            "\n",
            "Cosine_Similarity entre vector c y d:  [[0.53990119]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzSsyzo39ckj"
      },
      "source": [
        "Claramente, los vectores b y c son los más parecidos, aunque no en significado, pero sí en la composición de cada oración. Dado que en nuestro problema no tenemos que encontrar significados parecidos sino strings parecidas, es un método acertado para el problema. Aún así, tiene sus deficiencias este método porque, por ejemplo \"INDUSTRIA COMERCIAL\" será similar a \"COMERCIAL INDUSTRIAL\".  Una manera de resolver el orden de las palbras es separar las frases en _n-gramas_.\n",
        "\n",
        "El otro problema es que los vectores c y d son parecidos. Esto ocurre porque como el valor TF-IDF en el documento d para la palabra \"perro\" es alto, porque se repite muchas veces en el mismo documento, entonces el valor de **TF** sube, por ende aumenta el producto escalar entre los vectores, lo que eleva la similitud. De todas maneras, cuando el corpus es grande, es decir el número de palabras distintas en la colección total de documentos es elevado, el peso que adquiere **TF** en el producto **TF x IDF** es menor. \n",
        "\n",
        "El benficio, como se comentó anteriormente es la eficiencia en realizar cálculos matriciales, lo que permite aumentar la capacidad de cómputo. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0MmVqCesKMt"
      },
      "source": [
        "**Uso de los N-Grams ¿Por qué?**\n",
        "\n",
        "Para el análisis de similitud textual entre palabras, no referida al sentido, se puede hacer un análisis palabra a palabra o bien dividiendo las palabras en _n-gramas_. Estos son divisiones sucesivas de las palabras cada _n_ caracteres, por ejemplo: \n",
        "\n",
        "```\n",
        "ngrams('MINISTERIO', n=2)\n",
        "\n",
        "['MI', 'IN', 'NI', 'IS', 'ST', 'TE', 'ER', 'RI', 'IO']\n",
        "\n",
        "ngrams('MINISTERIO', n=3)\n",
        "\n",
        "['MIN', 'INI', 'NIS', 'IST', 'STE', 'TER', 'ERI', 'RIO']\n",
        "```\n",
        "Esto permite que ante problemas de mala escritura se puedan analizar de una misma palabra distintas partes de la misma. Esto permitiría una comparación más eficiente para el problema que tenemos en nuestro caso. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se1m8--0wPP8"
      },
      "source": [
        "**Eficiencia**\n",
        "\n",
        "A dififerencia de los problemas de comparación string contra string, el caso de TF-IDF es más eficiente. En este caso para analizar la similitud entre cada vector simplemente tenemos que hacer la multiplicación de la matriz de valores de TF-IDF por su traspuesta. Y luego evaluar cada valor con la similitud del coseno. Incluso hay librerías que han realizado implementaciones aún más performante. \n",
        "\n",
        "* Uso de CSR Matrix: \n",
        "\n",
        "La matriz CSR (Compressed Sparse Row) es un método compuacional que comprime el uso de memoria guardando los datos que no son ceros de una matriz, mediante el uso de indices de fila, de columna y de valores. \n",
        "![](https://op2.github.io/PyOP2/_images/csr.svg)\n",
        "\n",
        "\n",
        "* Selección de los strings más similares durante el proceso: \n",
        "\n",
        "El grupo de data scientists de la compañia ING, han implementado una optimización que permite obtener los strings más similares durante el proceso de computo guardando en memoria sólo las _n_ strings más similares([ver](https://medium.com/wbaa/https-medium-com-ingwbaa-boosting-selection-of-the-most-similar-entities-in-large-scale-datasets-450b3242e618)). \n"
      ]
    }
  ]
}